# ü¶ô Configuration Ollama pour le RAG Photographie

## Probl√®me

L'erreur indique que **Ollama n'est pas en cours d'ex√©cution** :

```
ConnectionRefusedError: [WinError 10061] Aucune connexion n'a pu √™tre √©tablie
HTTPConnectionPool(host='localhost', port=11434)
```

Le syst√®me essaie de se connecter √† Ollama sur le port 11434, mais le serveur n'est pas d√©marr√©.

## Solution : Installer et d√©marrer Ollama

### √âtape 1 : Installer Ollama

1. **T√©l√©charger Ollama** :
   - Aller sur https://ollama.com/download
   - T√©l√©charger la version Windows
   - Installer l'application

2. **V√©rifier l'installation** :
   ```bash
   ollama --version
   ```

### √âtape 2 : T√©l√©charger un mod√®le LLM

Une fois Ollama install√©, t√©l√©charge un mod√®le (par exemple `llama3`) :

```bash
ollama pull llama3
```

Cela peut prendre quelques minutes (le mod√®le fait plusieurs GB).

### √âtape 3 : D√©marrer Ollama

Ollama devrait d√©marrer automatiquement apr√®s l'installation. Si ce n'est pas le cas :

1. **Lancer Ollama manuellement** :
   - Chercher "Ollama" dans le menu D√©marrer
   - Lancer l'application
   - Ollama d√©marre en arri√®re-plan

2. **V√©rifier que Ollama fonctionne** :
   ```bash
   ollama list
   ```
   
   Tu devrais voir la liste des mod√®les t√©l√©charg√©s.

3. **Tester Ollama** :
   ```bash
   ollama run llama3 "Bonjour, comment √ßa va ?"
   ```

### √âtape 4 : Relancer le RAG

Une fois Ollama d√©marr√©, tu peux relancer :

```bash
python run_example.py
```

## Alternatives si Ollama ne fonctionne pas

### Option 1 : Utiliser un autre LLM local

Tu peux modifier `app/config.py` pour utiliser un autre LLM :

```python
# Au lieu de Ollama, utiliser un autre LLM
from langchain_community.llms import HuggingFacePipeline

llm = HuggingFacePipeline.from_model_id(
    model_id="mistralai/Mistral-7B-Instruct-v0.1",
    task="text-generation",
)
```

### Option 2 : Utiliser une API LLM (gratuite)

Tu peux utiliser des APIs gratuites comme :

- **HuggingFace Inference API** (gratuit avec limitations)
- **Groq** (gratuit, tr√®s rapide)
- **Together AI** (gratuit avec cr√©dits)

Exemple avec HuggingFace :

```python
from langchain_community.llms import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    endpoint_url="https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1",
    huggingfacehub_api_token="ton_token_ici",
)
```

### Option 3 : Mode test sans LLM

Pour tester le syst√®me sans LLM, tu peux modifier temporairement le code pour retourner juste les documents r√©cup√©r√©s.

## V√©rification rapide

Pour v√©rifier si Ollama fonctionne :

```bash
# V√©rifier que le serveur r√©pond
curl http://localhost:11434/api/tags
```

Ou dans PowerShell :

```powershell
Invoke-WebRequest -Uri http://localhost:11434/api/tags
```

Si tu obtiens une r√©ponse JSON, Ollama fonctionne !

## Configuration dans le projet

Le projet est configur√© pour utiliser Ollama par d√©faut avec le mod√®le `llama3`.

Tu peux changer le mod√®le dans `.env` :

```env
LLM_MODEL_NAME=llama3
```

Ou utiliser un autre mod√®le compatible Ollama :
- `llama3`
- `mistral`
- `llama2`
- `codellama`
- etc.

