# ‚ö° Optimisations RAG - R√©duction de la Latence

## üéØ Objectif
R√©duire le temps de r√©ponse moyen de **10 secondes √† 2-3 secondes**.

## ‚úÖ Optimisations Appliqu√©es

### 1. **R√©duction du nombre de documents** (gain: ~0.5-1s)
- **Avant**: 3 documents r√©cup√©r√©s
- **Apr√®s**: 2 documents par d√©faut
- **Configuration**: `NUM_RETRIEVAL_DOCS=2` dans `.env`

### 2. **Optimisation des prompts** (gain: ~1-2s)
- **Avant**: Prompt long et d√©taill√© (~300 caract√®res)
- **Apr√®s**: Prompt concis et optimis√© (~150 caract√®res)
- **Impact**: R√©duction du temps de traitement par le LLM

### 3. **Troncature intelligente du contexte** (gain: ~1s)
- **Avant**: Tout le contexte des documents r√©cup√©r√©s
- **Apr√®s**: Limit√© √† 1500 caract√®res maximum
- **Configuration**: `MAX_CONTEXT_LENGTH=1500` dans `.env`
- Chaque document limit√© √† 500 caract√®res

### 4. **R√©duction de la longueur de r√©ponse** (gain: ~1-2s)
- **Avant**: `num_predict=512` tokens
- **Apr√®s**: `num_predict=400` tokens
- **Impact**: G√©n√©ration plus rapide

### 5. **Timing d√©taill√©** (gain: visibilit√©)
- Ajout de logs d√©taill√©s pour identifier les goulots d'√©tranglement
- Mesure s√©par√©e pour:
  - Chargement du vector store
  - Recherche vectorielle
  - G√©n√©ration LLM

### 6. **Optimisation du streaming** (gain: perception)
- D√©lai de streaming r√©duit √† 0ms
- Pas d'attente artificielle entre les tokens

## üìä R√©sultats Attendus

| Composant | Avant | Apr√®s (attendu) |
|-----------|-------|-----------------|
| Vector store load | ~1-2s | ~0.1-0.5s (cache) |
| Recherche vectorielle | ~0.5-1s | ~0.3-0.5s |
| G√©n√©ration LLM | ~5-7s | ~2-3s |
| **TOTAL** | **~10s** | **~2-3s** |

## üîß Configuration

Ajoutez dans votre `.env`:

```env
# Nombre de documents √† r√©cup√©rer (2 par d√©faut, plus rapide)
NUM_RETRIEVAL_DOCS=2

# Taille maximale du contexte en caract√®res (1500 par d√©faut)
MAX_CONTEXT_LENGTH=1500

# D√©lai de streaming en secondes (0 par d√©faut pour plus de rapidit√©)
STREAMING_DELAY=0
```

## üìù Logs de Performance

Les logs affichent maintenant:

```
üì¶ Vector store charg√© en 125.50ms
üîç Recherche vectorielle en 340.20ms
‚ö° RAG r√©ponse g√©n√©r√©e en 2340.15ms (vector_store: 125.50ms, retrieval: 340.20ms, generation: 1874.45ms)
```

## üöÄ Prochaines Optimisations Possibles

1. **Cache des embeddings de requ√™tes** (gain: 2-3s)
   - Mettre en cache les embeddings des questions fr√©quentes
   - Utiliser Redis ou cache m√©moire

2. **Async/Await** (gain: 1-2s)
   - Parall√©liser les op√©rations ind√©pendantes
   - Utiliser `asyncio.gather()` pour les op√©rations parall√®les

3. **Mod√®le d'embedding plus rapide** (gain: 1-2s)
   - Utiliser un mod√®le plus l√©ger comme `all-MiniLM-L6-v2` (d√©j√† utilis√©)
   - Ou un mod√®le quantifi√©

4. **Optimisation de la recherche vectorielle** (gain: 0.5-1s)
   - Utiliser un index HNSW optimis√©
   - R√©duire la dimension des embeddings si possible

## ‚ö†Ô∏è Notes

- Ces optimisations r√©duisent l√©g√®rement la qualit√© des r√©ponses (moins de contexte)
- Le compromis vitesse/qualit√© peut √™tre ajust√© via les variables d'environnement
- Surveillez les logs pour identifier d'autres goulots d'√©tranglement
