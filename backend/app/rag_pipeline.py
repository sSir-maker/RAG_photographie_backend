from pathlib import Path
from typing import List, Optional
import time
import threading

from langchain_community.vectorstores import FAISS

from .config import settings
from .pipeline_components import (
    DocumentCollector,
    EmbeddingGenerator,
    OCRCorrector,
    OCREngine,
    OCRQualityMonitor,
    RAGGenerator,
    RetrievalEngine,
    SmartChunker,
    VectorStoreManager,
    analyze_document_structure,
)
from .monitoring_phoenix import get_phoenix_monitor
from .cache import get_cache_manager
import hashlib
import logging

logger = logging.getLogger(__name__)

# Cache global du vector store en m√©moire pour am√©liorer les performances
_vector_store_cache: Optional[FAISS] = None
_vector_store_lock = threading.Lock()
_vector_store_loading = False


def _build_vector_store_from_raw_documents(data_dir: Path) -> FAISS:
    """
    Impl√©mente ton pipeline MLOps OCR -> correction -> structuration -> chunking -> embeddings.
    """
    collector = DocumentCollector(root_dir=data_dir)
    ocr_engine = OCREngine()
    corrector = OCRCorrector()
    chunker = SmartChunker()
    embedder = EmbeddingGenerator()
    monitor = OCRQualityMonitor()

    docs = []

    for path in collector.get_documents():
        suffix = path.suffix.lower()
        base_metadata = {
            "source_document": path.name,
            "path": str(path),
            "date_extraction": None,
            "section_type": "texte",
        }

        if suffix in {".txt", ".md"}:
            raw_text = path.read_text(encoding="utf-8", errors="ignore")
            confidence = 1.0
        else:
            raw_text, confidence = ocr_engine.extract_text(path)
        if not raw_text:
            continue

        monitor.log_sample(confidence=confidence, source=path.name)
        cleaned_text = corrector.enhance_ocr_output(raw_text)
        structured = analyze_document_structure(cleaned_text)

        metadata = {
            **base_metadata,
            "confidence_ocr": confidence,
            "date_extraction": monitor.metrics[-1]["timestamp"],
        }

        docs.extend(chunker.create_chunks(structured, metadata))

    if not docs:
        raise RuntimeError("Aucun document exploitable n'a √©t√© trouv√© pour construire le vector store.")

    vector_store = embedder.generate_vectors(docs)

    vs_manager = VectorStoreManager(storage_dir=settings.vector_store_dir)
    vs_manager.save(vector_store)

    return vector_store


def _load_or_build_vector_store(force_rebuild: bool = False) -> FAISS:
    """
    Charge ou construit le vector store avec cache en m√©moire pour am√©liorer les performances.
    Le vector store est mis en cache en m√©moire apr√®s le premier chargement.
    """
    global _vector_store_cache, _vector_store_loading

    # Si on force la reconstruction, vider le cache
    if force_rebuild:
        with _vector_store_lock:
            _vector_store_cache = None

    # Si le cache existe, le retourner imm√©diatement (OPTIMISATION MAJEURE)
    if _vector_store_cache is not None:
        logger.debug("‚úÖ Utilisation du vector store en cache (beaucoup plus rapide)")
        return _vector_store_cache

    # √âviter les chargements multiples simultan√©s
    with _vector_store_lock:
        # V√©rifier √† nouveau apr√®s avoir acquis le lock
        if _vector_store_cache is not None:
            return _vector_store_cache

        if _vector_store_loading:
            # Attendre que le chargement se termine
            while _vector_store_loading:
                time.sleep(0.1)
            return _vector_store_cache

        _vector_store_loading = True

    try:
        logger.info("üì¶ Chargement du vector store depuis le disque...")
        start_time = time.time()

        vs_manager = VectorStoreManager(storage_dir=settings.vector_store_dir)
        if not force_rebuild and settings.vector_store_dir.exists():
            try:
                _vector_store_cache = vs_manager.load()
                load_duration = time.time() - start_time
                logger.info(f"‚úÖ Vector store charg√© en {load_duration:.2f}s (mis en cache)")
            except Exception as e:
                logger.warning(f"Erreur lors du chargement, reconstruction: {e}")
                _vector_store_cache = _build_vector_store_from_raw_documents(settings.data_dir)
                build_duration = time.time() - start_time
                logger.info(f"‚úÖ Vector store reconstruit en {build_duration:.2f}s (mis en cache)")
        else:
            # Force la reconstruction
            if force_rebuild and settings.vector_store_dir.exists():
                import shutil

                shutil.rmtree(settings.vector_store_dir)
            _vector_store_cache = _build_vector_store_from_raw_documents(settings.data_dir)
            build_duration = time.time() - start_time
            logger.info(f"‚úÖ Vector store reconstruit en {build_duration:.2f}s (mis en cache)")

        return _vector_store_cache
    finally:
        _vector_store_loading = False


def clear_vector_store_cache():
    """Vide le cache du vector store. Utile pour forcer un rechargement."""
    global _vector_store_cache
    with _vector_store_lock:
        _vector_store_cache = None
    logger.info("üóëÔ∏è Cache du vector store vid√©")


def answer_question(
    question: str, show_sources: bool = True, force_rebuild: bool = False, num_docs: Optional[int] = None
) -> dict:
    """
    Fonction utilitaire de haut niveau align√©e sur ton sch√©ma MLOps :
    - collecte + OCR + correction + structuration + chunking
    - embeddings + vector store
    - retrieval + g√©n√©ration RAG LangChain.

    Args:
        question: La question √† poser
        show_sources: Afficher les sources
        force_rebuild: Forcer la reconstruction du vector store
        num_docs: Nombre de documents √† r√©cup√©rer (None = utiliser la valeur de la config, d√©faut optimis√©: 3)

    Returns:
        dict avec 'answer' (r√©ponse) et 'sources' (documents utilis√©s)
    """
    # Utiliser la valeur de la config si non sp√©cifi√©e
    if num_docs is None:
        num_docs = settings.num_retrieval_docs

    # V√©rifier le cache si pas de force_rebuild
    cache = get_cache_manager()
    if not force_rebuild and cache.enabled:
        cache_key = f"rag:answer:{hashlib.md5(question.encode()).hexdigest()}"
        cached_result = cache.get(cache_key)
        if cached_result:
            logger.debug(f"Cache hit pour question: {question[:50]}...")
            return cached_result

    # R√©cup√©rer le monitor Phoenix
    monitor = get_phoenix_monitor()

    start_time = time.time()

    vector_store = _load_or_build_vector_store(force_rebuild=force_rebuild)
    retriever_engine = RetrievalEngine(vector_store)

    # R√©cup√©rer les documents pertinents AVANT de g√©n√©rer la r√©ponse
    retriever = retriever_engine.get_retriever()
    # OPTIMISATION: R√©duire le nombre de documents r√©cup√©r√©s pour plus de rapidit√© (3 au lieu de 4)
    if hasattr(retriever, "search_kwargs"):
        retriever.search_kwargs["k"] = num_docs
    else:
        retriever = vector_store.as_retriever(search_kwargs={"k": num_docs})

    retrieval_start = time.time()
    retrieved_docs = retriever.invoke(question)
    retrieval_duration = (time.time() - retrieval_start) * 1000  # ms

    # Monitor retrieval
    if monitor and monitor.enabled:
        sources_preview = []
        for doc in retrieved_docs[:5]:
            sources_preview.append(
                {"document": doc.metadata.get("source_document", "Inconnu"), "preview": doc.page_content[:100]}
            )
        monitor.trace_retrieval(question, sources_preview)

    # G√©n√©rer la r√©ponse avec le RAG
    rag_generator = RAGGenerator(retriever)
    generation_start = time.time()
    result = rag_generator.generate_answer(question)
    generation_duration = (time.time() - generation_start) * 1000  # ms

    # Extraire les sources r√©elles utilis√©es
    sources = []
    for doc in retrieved_docs:
        source_info = {
            "document": doc.metadata.get("source_document", "Inconnu"),
            "path": doc.metadata.get("path", ""),
            "page": doc.metadata.get("page", ""),
            "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
        }
        sources.append(source_info)

    answer = result.get("answer", "")
    total_duration = (time.time() - start_time) * 1000  # ms

    logger.info(
        f"‚ö° RAG r√©ponse g√©n√©r√©e en {total_duration:.2f}ms (retrieval: {retrieval_duration:.2f}ms, generation: {generation_duration:.2f}ms)"
    )

    # Monitor g√©n√©ration et pipeline complet
    if monitor and monitor.enabled:
        monitor.trace_generation(question, answer, model=settings.llm_model_name, duration_ms=generation_duration)

        monitor.trace_rag_pipeline(
            query=question,
            response=answer,
            documents_used=sources,
            metadata={
                "retrieval_duration_ms": retrieval_duration,
                "generation_duration_ms": generation_duration,
                "total_duration_ms": total_duration,
                "num_sources": len(sources),
            },
        )

    result = {
        "answer": answer,
        "sources": sources,
        "num_sources": len(sources),
    }

    # Mettre en cache le r√©sultat
    cache = get_cache_manager()  # R√©cup√©rer le cache manager
    if cache.enabled and not force_rebuild:
        cache_key = f"rag:answer:{hashlib.md5(question.encode()).hexdigest()}"
        cache.set(cache_key, result, ttl=3600)  # Cache 1h
        logger.debug(f"R√©sultat mis en cache: {cache_key}")

    return result


def answer_question_stream(
    question: str, force_rebuild: bool = False, num_docs: Optional[int] = None, streaming_delay: Optional[float] = None
):
    """
    Version streaming optimis√©e de answer_question qui g√©n√®re la r√©ponse token par token.
    Yields chaque token au fur et √† mesure pour permettre √† l'utilisateur de suivre le raisonnement.

    Args:
        question: La question √† poser
        force_rebuild: Forcer la reconstruction du vector store
        num_docs: Nombre de documents √† r√©cup√©rer (None = utiliser la valeur de la config, d√©faut optimis√©: 3)
        streaming_delay: D√©lai entre les tokens (None = utiliser la valeur optimis√©e)
    """
    try:
        from langchain_community.chat_models import ChatOllama

        use_chat_ollama = True
    except ImportError:
        from langchain_community.llms import Ollama

        use_chat_ollama = False
    from langchain_core.prompts import ChatPromptTemplate

    # Utiliser la valeur de la config si non sp√©cifi√©e
    if num_docs is None:
        num_docs = settings.num_retrieval_docs

    # OPTIMISATION: Utiliser un d√©lai de streaming r√©duit (5ms au lieu de 30ms par d√©faut)
    if streaming_delay is None:
        streaming_delay = min(0.005, settings.streaming_delay)  # Max 5ms pour plus de fluidit√©

    vector_store = _load_or_build_vector_store(force_rebuild=force_rebuild)
    retriever_engine = RetrievalEngine(vector_store)

    # R√©cup√©rer les documents pertinents AVANT de g√©n√©rer la r√©ponse
    retriever = retriever_engine.get_retriever()
    # OPTIMISATION: R√©duire le nombre de documents pour plus de rapidit√©
    if hasattr(retriever, "search_kwargs"):
        retriever.search_kwargs["k"] = num_docs
    else:
        retriever = vector_store.as_retriever(search_kwargs={"k": num_docs})

    retrieved_docs = retriever.invoke(question)

    # Pr√©parer les sources
    sources = []
    for doc in retrieved_docs:
        source_info = {
            "document": doc.metadata.get("source_document", "Inconnu"),
            "path": doc.metadata.get("path", ""),
            "page": doc.metadata.get("page", ""),
            "preview": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
        }
        sources.append(source_info)

    # Construire le contexte √† partir des documents r√©cup√©r√©s
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # Cr√©er le prompt avec le contexte
    system_prompt = """
Tu es un expert en photographie (prise de vue, lumi√®re, composition, mat√©riel, post‚Äëtraitement).
Tu dois r√©pondre **en fran√ßais**, avec des explications claires et des conseils concrets :
- propose des r√©glages (ISO, ouverture, vitesse, focale) adapt√©s √† la situation
- prends en compte que le contexte provient d'un OCR et peut contenir de petites erreurs
- cite les sources (fichier, num√©ro de page si disponible)
- si l'information n'est pas dans le contexte, dis‚Äële honn√™tement.

Contexte :
{context}
"""
    prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])

    # Formater le prompt avec le contexte et la question
    formatted_prompt = prompt.format_messages(context=context, input=question)

    # OPTIMISATION: Cr√©er le LLM pour le streaming avec param√®tres optimis√©s pour vitesse
    if use_chat_ollama:
        llm = ChatOllama(
            model=settings.llm_model_name,
            temperature=0.7,
            num_predict=512,  # Limiter la longueur de r√©ponse pour plus de rapidit√©
        )
    else:
        llm = Ollama(
            model=settings.llm_model_name,
            temperature=0.7,
            num_predict=512,
        )

    full_answer = ""

    try:
        # Streamer directement depuis le LLM pour avoir les tokens un par un
        # Le streaming retourne des chunks (peut contenir un ou plusieurs tokens)
        for chunk in llm.stream(formatted_prompt):
            # Extraire le contenu du chunk
            token = None

            if hasattr(chunk, "content"):
                # AIMessageChunk ou similaire
                token = chunk.content
            elif hasattr(chunk, "text"):
                # Chunk avec attribut text
                token = chunk.text
            elif isinstance(chunk, str):
                # Chunk est directement une string
                token = chunk
            elif isinstance(chunk, dict):
                # Chunk est un dictionnaire
                token = chunk.get("content", chunk.get("text", chunk.get("token", "")))
            else:
                # Essayer de convertir en string
                token = str(chunk) if chunk else ""

            if token:
                full_answer += token
                yield token
                # OPTIMISATION: D√©lai r√©duit pour plus de rapidit√© et fluidit√©
                if streaming_delay > 0:
                    time.sleep(streaming_delay)

    except Exception as e:
        # Si le streaming √©choue, g√©n√©rer la r√©ponse normalement et la streamer caract√®re par caract√®re
        print(f"Streaming direct √©chou√©, utilisation du fallback: {e}")
        import traceback

        traceback.print_exc()
        try:
            # Fallback : g√©n√©rer la r√©ponse compl√®te puis la streamer
            rag_generator = RAGGenerator(retriever)
            result = rag_generator.generate_answer(question)
            full_answer = result.get("answer", "")

            # Streamer caract√®re par caract√®re pour simuler le streaming
            for char in full_answer:
                yield char
                if streaming_delay > 0:
                    time.sleep(streaming_delay)
        except Exception as e2:
            raise RuntimeError(f"Erreur lors de la g√©n√©ration: {str(e2)}") from e2

    # Retourner les sources √† la fin
    yield {"sources": sources, "full_answer": full_answer}
